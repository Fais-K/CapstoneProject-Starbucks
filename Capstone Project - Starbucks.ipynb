{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Objective\nThrough this project, I am going to build a machine learning model that\npredicts whether or not a customer will respond to a purchase offer sent by Starbucks through their mobile app. And, to determine the possible\nlevel of response or user actions like offer received, offer viewed, transaction, offer\ncompleted etc. Such that, the company can send each offer to the respective targeted audience where it can get the possible maximum reponse as predicted. I will be using the dataset 'Starbucks app customer rewards program data' which contains simulated data that mimics\ncustomer behaviour on the Starbucks rewards mobile app for this machine learning problem.\n\n### Possible User Actions or Responses:\n1. **offer recieved** - Indicates that the offer has reached the customer.\n2. **offer viewed** - Indicates that the customer has viewed the offer.\n3. **transaction** - Indicates that the customer has performed a transaction but were not eligible for the offer.\n4. **offer completed** - Indicates that the customer has performed a transaction using the sent offer.\n5. **green flag** - Indicates that the customer has performed a transaction without using the offer, eventhough they were eligible for the offer.(From a business perspective, it's a good sign. We could restrict senting offers to those people in the future.)\n\n\n# Problem Statement\nPredict the purchace offer to which a possible higher level of response or user actions like ‘offer\nreceived’, ‘offer viewed’, ‘transaction’, ‘offer completed’, ‘green flag’ (in order from the\ntop priority to low) can be achieved based on the demographic attributes of the\ncustomer and other attributes of the companies purchase offers. If the customer response is not the\nmaximum i.e. ‘green flag’ then execute the preset procedures to elevate the response\nfrom its current value to the next higher possible value."},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport math\nimport json\n\n# read in the json files\nportfolio = pd.read_json('../input/starbucks-app-customer-reward-program-data/portfolio.json', orient='records', lines=True)\nprofile = pd.read_json('../input/starbucks-app-customer-reward-program-data/profile.json', orient='records', lines=True)\ntranscript = pd.read_json('../input/starbucks-app-customer-reward-program-data/transcript.json', orient='records', lines=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Dataset Formation\n\nFirst of all, let's discuss what is so strange about our dataset and how to handle it. The Starbucks dataset exists as three separate files i.e. portfolio.json (), profile.json and transcript.json. So, we have to identify a way to combine the required data variables from these data files to form a single dataset.  \n"},{"metadata":{},"cell_type":"markdown","source":"To start with, let's check whether the unique values in profile equals unique values in transcript to make sure that there are no additional customer records in the profile which are not in transcript."},{"metadata":{"trusted":true},"cell_type":"code","source":"if(pd.unique(transcript['person']).shape == pd.unique(profile['id']).shape):\n    print(\"Success\")\nelse:\n    print(\"Failure\")","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"customer_ids = pd.unique(transcript['person'])\n\n#encode customer ids which is in string format to integers\ncustomer_ids_dict = pd.Series(customer_ids).to_dict()\ncustomer_ids_dict = dict([(value, key) for key, value in customer_ids_dict.items()]) \nitr = iter(customer_ids_dict.items())\nlst = [next(itr) for i in range(10)]\nprint(lst)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#map encoded customer ids to ids in transcrpt and profile dataframes\ntranscript['person'] = transcript['person'].map(customer_ids_dict)\nprofile['id'] = profile['id'].map(customer_ids_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"profile.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"transcript.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"portfolio.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#sort transcript and profile dataframe rows based on customer ids\nsorted_transcript = transcript.sort_values('person', axis=0, ascending=True, inplace=False, kind='quicksort')\nsorted_profile = profile.sort_values('id', axis=0, ascending=True, inplace=False, kind='quicksort')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#reset index to the current form i.e. after sort\nsorted_transcript.reset_index(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#drop the column index as it is not required\nsorted_transcript = sorted_transcript.drop(labels=['index'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sorted_transcript.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#reset index to the current form i.e. after sort\nsorted_profile.reset_index(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#drop the column index as it is not required\nsorted_profile = sorted_profile.drop(labels=['index'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sorted_profile.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#find frequency of each customer ids in the transcript dataframe\ncustomer_ids_frequency = sorted_transcript['person'].value_counts(sort=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#to perform repeatation of records, the repeat count is to be added along each corresponding rows\nsorted_profile = pd.concat([sorted_profile, customer_ids_frequency], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sorted_profile.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#duplicate each rows in sorted profile based on the frequency of each customer ids in transcript\nprofile_with_duplicate_rows = sorted_profile.reindex(sorted_profile.index.repeat(sorted_profile.person))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#reset index to the current form i.e. after sort\nprofile_with_duplicate_rows.reset_index(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#drop the columns index and person as they aren't anymore required\nprofile_with_duplicate_rows = profile_with_duplicate_rows.drop(labels=['index', 'person'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"profile_with_duplicate_rows.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#concatenate sorted transcript and profile with duplicate rows\ntranscript_profile_concatenated = pd.concat([sorted_transcript, profile_with_duplicate_rows], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"transcript_profile_concatenated.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#verify whether customer ids from transcript and profile are aligned correctly\n(transcript_profile_concatenated['person'] == transcript_profile_concatenated['id']).all()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#drop the column person as it is not anymore required\ntranscript_profile_concatenated = transcript_profile_concatenated.drop(labels=['person'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"transcript_profile_concatenated","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"portfolio.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#encode offer ids in portfolio dataframe from string format to integer\noffer_ids = portfolio['id'].unique()\noffer_ids_dict = pd.Series(offer_ids).to_dict()\noffer_ids_dict = dict([(value, key) for key, value in offer_ids_dict.items()]) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"offer_ids_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#map ids in portfolio to encoded offer ids\nportfolio['id'] = portfolio['id'].map(offer_ids_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"portfolio","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"#inspect random rows for any error\nfraction_of_rows = transcript_profile_concatenated.sample(frac=0.003)\nfraction_of_rows","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"transcript_profile_concatenated.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"transcript_profile_concatenated.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#get a series of offer ids from the concatenated dataframe\noffer_id_series = transcript_profile_concatenated['value']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_dict_values(x):\n    \"\"\"Finds the first value of the key from a single key-value pair.\n    \n    Args:\n        x (dictionary object): Expects a dictionary key-value pair.\n    \n    Returns:\n        value: First value from the passed key-value pair.\n        \n    \"\"\"\n    key = list(x.keys())[0]\n    value = x[key]\n    return value","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#map every element of offer id series to the function get_dict_values(). This step is essential as every \n#element in offer id series exists as a dictionary key-value pair.\noffer_id_series = pd.DataFrame(offer_id_series).applymap(lambda x: get_dict_values(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"offer_id_series.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def encode_offer_id(x):\n    \"\"\"Encode the given string into a integer based on the dictionary offer_ids_dict.\n    \n    Args:\n        x (str or int): Expects an integer or string value from offer_id_series.\n    \n    Return:\n        10 or offer_id_dict[x]: 10 is returned if the passed argument is integer and if the argument is a string,\n                                the value for the key, x in offer_id_dict is returned.\n    \"\"\"\n    if(type(x) is str):\n        return offer_ids_dict[x]\n    else:\n        return 10","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#map offer id series to the function encode_offer_id()\nencoded_offer_id_series = offer_id_series.applymap(lambda x: encode_offer_id(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoded_offer_id_series.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#add the column, offer_id with values from encoded_offer_id_series dataframe to the dataframe \n#transcript_profile_portfolio_concatenated\ntranscript_profile_portfolio_concatenated = transcript_profile_concatenated\ntranscript_profile_portfolio_concatenated['offer_id'] = encoded_offer_id_series['value']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"transcript_profile_portfolio_concatenated.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"portfolio.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#convert the columns reward, difficulty and duration of portfolio dataframe into dictionaries \nportfolio_reward_dict = portfolio['reward'].to_dict()\nportfolio_difficulty_dict = portfolio['difficulty'].to_dict()\nportfolio_duration_dict = portfolio['duration'].to_dict()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def add_column(column_name, column_dict):\n    \"\"\"To add new column to the transcript_profile_portfolio_concatenated dataframe.\n    \n    Args:\n        column_name (str): Name of the column to be added.\n        column_dict (dict): Dictionary with column name as the key and values as data to the column.\n        \n    Return:\n        None\n    \"\"\"\n    transcript_profile_portfolio_concatenated[column_name] = transcript_profile_portfolio_concatenated['offer_id'].map(column_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"add_column('reward', portfolio_reward_dict)\nadd_column('difficulty', portfolio_difficulty_dict)\nadd_column('duration', portfolio_duration_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"transcript_profile_portfolio_concatenated.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#inspect random rows to find the basic structure of the column became_member_on\ntranscript_profile_portfolio_concatenated['became_member_on'].sample(frac=.003)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def find_month(x):\n    \"\"\"To find month from the passed numerical date of the format YYYYMMDD.\n    \n    Args:\n        x (int): Date which is of the form YYYYMMDD.\n        \n    Return:\n        x%100 (int): MM of the date is returned.\n    \"\"\"\n    x = x/100\n    x = int(x)\n    return x%100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#map the column reg_month to the find_month() function\ntranscript_profile_portfolio_concatenated['reg_month'] = transcript_profile_portfolio_concatenated['became_member_on'].map(lambda x: find_month(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"transcript_profile_portfolio_concatenated.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#encode event ids in string format to integer\nevent_ids = transcript_profile_portfolio_concatenated['event'].unique()\nevent_ids_dict = pd.Series(event_ids).to_dict()\nevent_ids_dict = dict([(value, key) for key, value in event_ids_dict.items()]) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#add one more event to the dictionary, green flag which indicates the customer does not view the \n#offer but completes the offer. Thus, it's a green flag in a business perspective.\nevent_ids_dict['green flag'] = 4\nevent_ids_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#map event_ids to the encoded event ids\ntranscript_profile_portfolio_concatenated['event_id'] = transcript_profile_portfolio_concatenated['event'].map(event_ids_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"transcript_profile_portfolio_concatenated","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#sort the transcript_profile_portfolio_concatenated dataframe based on id, offer_id and event_id for easy feature engineering later\nsorted_dataset = transcript_profile_portfolio_concatenated.sort_values(by=['id', 'offer_id', 'event_id'], ascending=[True, True, False])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sorted_dataset.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#reset index to the current form i.e. after sort\nsorted_dataset.reset_index(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#form the dataset with only columns that are required for later cases\ncolumns=['id', 'offer_id', 'event_id', 'gender', 'age', 'income', 'reward', 'difficulty', 'reg_month']\ndataset = sorted_dataset[columns]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#dataframe for storing dataframe from dataset after filtering out unwanted rows\ndataset_after_filter = pd.DataFrame()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def aggregate(x):\n    \"\"\"To find sum of the current element and all the previous elements and storing it in the current position.\n       This is to be done from start of the list to the end.\n       \n    Args:\n        x (list): List of integer values.\n        \n    Returns:\n        x (list): Function will be performed to every element from start to end. Last element will be removed \n        from the list before returning.\n    \n    \"\"\"\n    for i in range(1, len(x)):\n        x[i] = x[i] + x[i-1]\n    x.pop()\n    return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.simplefilter('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\n#find and select only the records from the dataset for which the event performed by the customer for each offers\n#send to them is the highest. And, change the event id of those selected records with event id = 1 into \n#event id = 2 if there is a transaction event happened for the same customer; otherwise, it's not changed. \n#Similarly, if in the selected records, if there is event id = 3 and the event id of the next data record in the \n#dataset is zero then the erecord with event id = 3 is changed to event id = 4\n\n#loop through each customer ids\nfor i in range(len(dataset['id'].unique())):    \n    #select the data records of i th customer from dataset \n    dataset_temp = dataset[dataset['id'] == i]\n    customer_id_index = dataset_temp['id'].index\n    #start index of dataframe of i th customer\n    start_index = customer_id_index[0] \n    \n    #find the frequency of each offer ids such that sorting of these values is set to false\n    offer_count = dataset_temp['offer_id'].value_counts(sort=False)\n    #find the index of offer ids with just a single record\n    single_offer_index = offer_count[offer_count == 1].index\n    #add 0 to the 0th index of the offer_count_list \n    offer_count_list = offer_count.to_list()\n    offer_count_list.insert(0, 0)\n    offer_index = pd.Series(aggregate(offer_count_list))\n    #start index for each customer while each iteration is added\n    offer_index = offer_index.apply(lambda x: x + start_index)\n    \n    #check whether 1 exist at any indexes specified by the series offer_index and if there is any, store \n    #those indexes to the variable event_one_exist.\n    event_one_exist = dataset.iloc[offer_index, dataset.columns.get_loc('event_id')] == 1\n    event_one_exist = event_one_exist[event_one_exist==1].index\n    \n    #check whether 10 exist in the i th dataframe and if there is any, store those indexes to the \n    #variable offer_ten_exist\n    offer_ten_exist = dataset_temp[dataset_temp['offer_id']==10].index\n    \n    #when index of offer_count_list increased by one by adding zero, offer count hadn't increased the index.\n    if(len(single_offer_index)!=0):\n        single_offer_index = list(single_offer_index.map(lambda x: x+1))\n    \n    #check whether 3 exist at any indexes specified by the series offer_index and if there is any, store \n    #those indexes to the variable event_three_exist\n    event_three_exist = dataset.iloc[offer_index, dataset.columns.get_loc('event_id')] == 3\n    event_three_exist = event_three_exist[event_three_exist==1].index\n    \n    #find if the records specified in event_three_exist is a single offer record or not; if yes, pass the \n    #common value to the common_index variable\n    common_index = set(event_three_exist).intersection(single_offer_index)\n    common_index = list(common_index)\n    \n    #if there is any elements in common_index, remove those from the event_three_exist variable\n    if(len(common_index)!=0):\n        indices_B = [event_three_exist.index(x) for x in common_index]\n        event_three_exist = [i for j, i in enumerate(event_three_exist) if j not in indices_B]\n        \n    #pointer to the record next to the records with event_id = 3\n    event_three_next = pd.Series(event_three_exist).apply(lambda x: x+1)\n    \n    #check whether 0 exist at any indexes specified by the series event_three_next and if there is any, store \n    #those indexes to the variable event_zero_exist\n    event_zero_exist = dataset.iloc[event_three_next, dataset.columns.get_loc('event_id')] == 0\n    event_zero_exist = event_zero_exist[event_zero_exist==1].index\n\n    #If any event_id = 1 and offer ten exist, then the event_ids are made to 2 for all index values in \n    #event_one_exist\n    if(len(event_one_exist)!=0 and len(offer_ten_exist)!=0):\n        dataset.iloc[event_one_exist, dataset.columns.get_loc('event_id')] = 2\n    \n    #If any event_id = 3 and the next record's event_id = 0, then the event_ids are made to 4 for all \n    #index values in event_zero_exist(values of event_zero_exist are reduced by one)\n    if(len(event_three_exist)!=0 and len(event_zero_exist)!=0):\n        event_zero_exist = pd.Series(event_zero_exist).apply(lambda x: x-1)\n        dataset.iloc[list(event_zero_exist), dataset.columns.get_loc('event_id')] = 4    \n    \n    #For each iteration or customer ids, records of the dataset at the indexes specified by offer_index is appended\n    #to the dataframe dataset_after_filter\n    dataset_after_filter = dataset_after_filter.append(dataset[dataset.index.isin(offer_index)], ignore_index=True)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset_after_filter.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset_after_filter.head(100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#as offer_id = 10 doesn't actually represent any offer by itself, all the rows containing offer_id = 10 is dropped\ndataset_after_filter = dataset_after_filter.drop(dataset_after_filter[dataset_after_filter['offer_id']==10].index, axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset_after_filter.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset_after_filter.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#reset index to the current form i.e. after sort\ndataset_after_filter.reset_index(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#drop the column index as it is not required\ndataset_after_filter = dataset_after_filter.drop(labels=['index'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset_after_filter.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#encode the column 'gender' in the string format to integer\ngender_dict = {'O': 0, 'M': 1, 'F': 2}\ndataset_after_filter['gender'] = dataset_after_filter['gender'].map(gender_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset_after_filter","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset_after_filter.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset_after_filter.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset_after_filter.to_csv('data.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis (EDA)\n\n\n## 1. Variable Identification\n\nIdentify the types and categories of all the data variables. \n\nFirst, let's inspect the data variables that exists in the three files that is available to us from Starbucks."},{"metadata":{"trusted":true},"cell_type":"code","source":"portfolio.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have reward (int), difficulty (int) and duration (int) as continuous values; where as channels (list of strings), offer_type (str) and id (str) as categorical data."},{"metadata":{"trusted":true},"cell_type":"code","source":"profile.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have age (int), became_member_on (int) and income (float) as continuous values; where as gender (str) and id (str) as categorical data."},{"metadata":{"trusted":true},"cell_type":"code","source":"transcript.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have time (int) as continuous value; where as person (str), event (str) and value (dict of strings) as categorical data."},{"metadata":{},"cell_type":"markdown","source":"### The identified input and output features/variables for our model are:\n\n#### Predictor variables:\n\n -  Difficulty\n -  Reward\n -  Gender\n -  Age\n -  Became_member_on (Only the month is considered and the feature name is changed to reg_month)\n -  Income\n -  Value (Feature name is changed to offer_id)\n\n#### Target:\n\n -  Event (Feature name is changed to event_id)\n \n "},{"metadata":{},"cell_type":"markdown","source":"### Let's import our combined dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('data.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here, we have all the output and input features combined together and an additional variable 'id' which is nothing but the customer id."},{"metadata":{},"cell_type":"markdown","source":"## 2. Missing Value Treatment\n\nMissing data in the training dataset can reduce the power / fit of a model or can lead to a biased model because we have not analysed the behavior and relationship with other variables correctly. It can lead to wrong prediction or classification.\n\nLet's analyse the missing values in our dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Find the sum of all null or missing values in each variable of the dataset\ndata.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 8,078 missing values in each of the data variables 'gender' and 'income' (Both are part of the same record) of our dataset. Let's analyze these records alone to identify the reasons for occurrence of these missing values and the ways to treat it."},{"metadata":{"trusted":true},"cell_type":"code","source":"data[data['gender'].isna()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above dataframe we can understand that even the age from these records are quite unreal. It's most probable that those missing records are because customers were not ready to submit the details and hence, the gender and income became missing values and age got its set default value i.e. 118.\n\nReplacing missing values with mean/mode/median wouldn't be a good choice; as there are a large amount of records with missing values, mean/mode/median imputation could convey false information to the model. Since there are three features to be identified KNN imputation would also be riskful and inefficient. Thus, the better option would be to delete those records. (Even after deleting those records with missing value, we will have 55296 records for modeling)"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data.dropna()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#reset the index \ndata.reset_index(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#drop the column index as it is not required\ndata = data.drop(labels=['index'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Univariate Analysis\n\nAt this stage, we explore variables one by one. Method to perform uni-variate analysis will depend on whether the variable type is categorical or continuous.\n\nContinuous Variables:- We need to understand the central tendency and spread of the variable. \n\nCategorical Variable:- We’ll use frequency table to understand distribution of each category.\n\n\n"},{"metadata":{},"cell_type":"markdown","source":"### 3.1. Central Tendency \n\nThe mean is most likely the measure of central tendency, but there are others, such as\nthe median and the mode. One of the main disadvantages of mean is that it is\nparticularly susceptible to the influence of outliers. Since our dataset has no outliers we\nmay use the mean and also, the histogram to visualize the same.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from matplotlib import pyplot as plt\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Converting each column into numpy\nincome_np = data['income'].to_numpy()\noffer_id_np = data['offer_id'].to_numpy()\nevent_id_np = data['event_id'].to_numpy()\nage_np = data['age'].to_numpy()\ngender_np = data['gender'].to_numpy()\nreward_np = data['reward'].to_numpy()\ndifficulty_np = data['difficulty'].to_numpy()\nreg_month_np = data['reg_month'].to_numpy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=[10, 8])\nn, bins, patches = plt.hist(x=income_np, bins='auto', color='#0504aa',alpha=0.7, rwidth=0.85)\nplt.xlabel('Value',fontsize=15)\nplt.ylabel('Frequency',fontsize=15)\nplt.title('Data Distribution Histogram - Income',fontsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This histogram gives us the idea that our dataset lacks records or datapoints of customers who has income between 70,000 and 120,000. It's essentially a distribution which is positively skewed or right skewed. So in skewed data, the tail region may act as an outlier for the statistical model and we know that outliers adversely affect the model’s performance. Hence, we may need to normalize the data afterwards."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=[10, 8])\nn, bins, patches = plt.hist(x=age_np, bins='auto', color='#0504aa',alpha=0.7, rwidth=0.85)\nplt.xlabel('Value',fontsize=15)\nplt.ylabel('Frequency',fontsize=15)\nplt.title('Data Distribution Histogram - Age',fontsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The distribution of the variable ‘age’\nin the dataset is positively skewed or right skewed. Hence, we may need to standardize\nthe same."},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=[10, 8])\nn, bins, patches = plt.hist(x=reg_month_np, bins='auto', color='#0504aa',alpha=0.7, rwidth=0.85)\nplt.xlabel('Value',fontsize=15)\nplt.ylabel('Frequency',fontsize=15)\nplt.title('Data Distribution Histogram - Registration Month',fontsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As the data distribution cannot be assumed as a gaussian distribution, we\nmay not standardise the variable but we may normalize it to a normal scale such that there won’t be varying scales in the dataset and the algorithm we are using does not\nmake assumptions about the distribution of the data."},{"metadata":{"trusted":true},"cell_type":"code","source":"unique_elements, counts_elements = np.unique(reward_np, return_counts=True)\n\nplt.figure(figsize=[10, 8])\np = plt.bar(unique_elements, counts_elements, color='#0504aa',alpha=0.7, width=.4)\nplt.xlabel('Value',fontsize=15)\nplt.ylabel('Frequency',fontsize=15)\nplt.title('Categorical Distribution Bar Chart - Reward',fontsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Normalization is to be done."},{"metadata":{"trusted":true},"cell_type":"code","source":"unique_elements, counts_elements = np.unique(difficulty_np, return_counts=True)\n\nplt.figure(figsize=[10, 8])\np = plt.bar(unique_elements, counts_elements, color='#0504aa',alpha=0.7, width=.4)\nplt.xlabel('Value',fontsize=15)\nplt.ylabel('Frequency',fontsize=15)\nplt.title('Categorical Distribution Bar Chart - Difficulty',fontsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Normalization is to be done."},{"metadata":{},"cell_type":"markdown","source":"### 2.2. Frequency Table"},{"metadata":{"trusted":true},"cell_type":"code","source":"unique_elements, counts_elements = np.unique(offer_id_np, return_counts=True)\n\nplt.figure(figsize=[12, 8])\np = plt.bar(unique_elements, counts_elements, color='#0504aa',alpha=0.7, width=.4)\nplt.xlabel('Category',fontsize=15)\nplt.ylabel('Frequency',fontsize=15)\nplt.title('Categorical Distribution Bar Chart - Offers',fontsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have a balanced categorical data distribution\nfor the variable ‘offers’ which can be encoded later using any suitable encoding scheme."},{"metadata":{"trusted":true},"cell_type":"code","source":"unique_elements, counts_elements = np.unique(gender_np, return_counts=True)\n\nplt.figure(figsize=[8, 8])\np = plt.bar(unique_elements, counts_elements, color='#0504aa',alpha=0.7, width=.4)\nplt.xlabel('Category',fontsize=15)\nplt.ylabel('Frequency',fontsize=15)\nplt.title('Categorical Distribution Bar Chart - Gender',fontsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have an imbalanced categorical data\ndistribution for the variable ‘gender’ which can be encoded later using any suitableencoding scheme. The first category (‘other’) is the least occuring and hence, its\ninfluence on the target class can be assumed to be low. Also, just because the variable\nhas a class imbalance, doesn't necessarily mean it isn't correlated with the target\nvariable."},{"metadata":{"trusted":true},"cell_type":"code","source":"unique_elements, counts_elements = np.unique(event_id_np, return_counts=True)\n\nplt.figure(figsize=[10, 8])\np = plt.bar(unique_elements, counts_elements, color='#0504aa',alpha=0.7, width=.4)\nplt.xlabel('Category',fontsize=15)\nplt.ylabel('Frequency',fontsize=15)\nplt.title('Categorical Distribution Bar Chart - Events',fontsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Our dataset is imbalanced. The event 'offer viewed' and 'green flag' are the least occuring events compared to the other events. "},{"metadata":{},"cell_type":"markdown","source":"## 4. Bi-variate Analysis\n\nBi-variate Analysis finds out the relationship between two variables. Here, we look for association and disassociation between variables at a pre-defined significance level. We can perform bi-variate analysis for any combination of categorical and continuous variables.\n\nWe will be considering the following combination of variables:\n\n1. Age & Income.\n2. Offer ID & Registration Month\n3. Event ID & Gender"},{"metadata":{},"cell_type":"markdown","source":"### 4.1. Categorical & Categorical\n\nTo find the relationship between two categorical variables\n\n - Event ID & Gender"},{"metadata":{"trusted":true},"cell_type":"code","source":"event_ids = np.unique(event_id_np)\n\nother_list = np.empty(shape=0, dtype=np.int64)\nmale_list = np.empty(shape=0, dtype=np.int64)\nfemale_list = np.empty(shape=0, dtype=np.int64)\n\nfor i in range(len(event_ids)):\n    gender_count = data[data['event_id']==i].gender.value_counts(sort=False)\n    \n    other_list = np.append(other_list, gender_count[0])\n    male_list = np.append(male_list, gender_count[1])\n    female_list = np.append(female_list, gender_count[2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=[12, 8])\np1 = plt.bar(event_ids, male_list, alpha=0.5, color='#8b008b')\np2 = plt.bar(event_ids, female_list, bottom=male_list, alpha=0.5, color='#ffe4e1')\np3 = plt.bar(event_ids, other_list, bottom=male_list+female_list, alpha=0.5, color='#4682b4')\nplt.ylabel('Scores', fontsize=15)\nplt.title('Scores on Events & Gender', fontsize=15)\nplt.legend((p1[0], p2[0], p3[0]), ('Male', 'Female', 'Others'))\nplt.xticks(event_ids, ('Event 1', 'Event 2', 'Event 3', 'Event 4', 'Event 5'))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No correlation."},{"metadata":{},"cell_type":"markdown","source":"### 4.2. Continuous & Continuous\n\nTo find the relationship between two continuous variables.\n\n - Age & Income"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=[10, 8])\nplt.scatter(income_np, age_np, color='#0504aa', alpha=0.5)\nplt.title('Scatter Plot - Income & Age')\nplt.xlabel('Income')\nplt.ylabel('Age')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can observe from the figure that there is a stepwise increase in the income as the age value increases. But we may not be bothered about the same as it’s not a linear correlation."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=[10, 8])\nplt.scatter(income_np, age_np, color='#0504aa', alpha=0.5)\nplt.title('Scatter Plot - Income & Age')\nplt.xlabel('Income')\nplt.ylabel('Age')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.3. Continuous & Categorical\n\nTo find the relationship between a continuous and a catagorical variables.\n\n - Offer ID & Registration Month"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=[10, 8])\ndata.boxplot(column='reg_month', by='offer_id', ax=ax, grid=False, fontsize=15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Correlation among the variables can be inferred from the graph, thus we may try\nmodeling without and with the variable ‘reg_month’. We can also infer that there are no\noutliers."},{"metadata":{},"cell_type":"markdown","source":"## 6. Outliers Detection"},{"metadata":{},"cell_type":"markdown","source":"Let’s use the mathematical function ‘Z-score’ to detect the outliers in the dataset. While\ncalculating the ‘Z-score’ we re-scale and center the data and look for data points which\nare too far from zero. These data points which are way too far from zero will be treated\nas the outliers. We have set a threshold of 3 and -3 i.e. if the Z-score value is greater\nthan or less than 3 or -3 respectively, that data point will be identified as outliers."},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy import stats","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"z_score_data = np.abs(stats.zscore(data))\nthreshold = 3\nprint(np.where(z_score_data > 3))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There aren't any outliers to treat using the mathematical function z-score and from the above visualizations."},{"metadata":{},"cell_type":"markdown","source":"## 5. Multi-dimensional Scaling"},{"metadata":{},"cell_type":"markdown","source":"Multidimensional scaling is a means of visualizing the level of similarity of individual objects of a dataset. It is used to translate information about the pairwise distances among a set of n objects or individuals into a configuration of n points mapped into an abstract Cartesian space. \n\nWe may use this method to scale our dataset dimension into just two components such that the relative distance between the data points are maintained. And then we may scatter plot each class in the dataset into the x-y plane. This may give us a narrow idea about the underlying structure of each class in the dataset.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.manifold import MDS","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#seperate the inputs and output from the dataset.\ntarget = data['event_id']\npredictors = data.drop(['id', 'event_id'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def scatter_plot(event, event_name):\n    \"\"\"To plot records of the passed class or event.\n    \n    The function perform MDS on the data and scatter plot the same into the 2d space.\n    \n    Args : \n        \n        event (dataframe) - It's the records of the corresponding event.\n        event_name (str) - Specifies event name. \n           \n    Returns:\n        \n        None.\n        \n    \"\"\"\n    sample = event.head(600)\n    clf = MDS(n_components=2, n_init=2, max_iter=100, dissimilarity='euclidean')\n    X_mds = clf.fit_transform(sample.values)\n    cords = X_mds\n    scatter = plt.scatter(cords[:, 0], cords[:, 1], label=event_name)  \n\nplt.figure(figsize=[10, 8])\n\n#selecting records corresponding to each event or class and passing it to the function 'scatter_plot'\nclass_name = ['Class 1', 'Class 2', 'Class 3', 'Class 4', 'Class 5']\nfor i in range(5):\n    event = predictors[target==i]\n    scatter_plot(event, class_name[i])\n    \nplt.legend()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the figure, we can observe that each class in the dataset is separable and is suitable for classification problems. And, from the structure of the data points we can infer that we may need to use a nonlinear function or model to solve the same.\n"},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering\n\nIn Feature Engineering we are not going to add any new data, but we are going to make the data that we already have useful."},{"metadata":{},"cell_type":"markdown","source":"## 1. Variable Standardization & Normalization\n\nWe may use this methods for many reasons such as to scale the data variable, to transform complex non-linear relationships into linear relationships, to normalize skewed distributions etc. \n\nAs we have analyzed in the univariate analysis stage, some of our data variables requires standardization so as to avoid its skewed nature and some requires normalization for scaling down the data."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#split the dataset into test and train sets.\nX_train, X_test, y_train, y_test = train_test_split(predictors, target, test_size=0.25, random_state=111)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"std = preprocessing.StandardScaler()\nX_train.income = std.fit_transform(X_train.income.values.reshape(-1, 1))\nX_test.income = std.transform(X_test.income.values.reshape(-1, 1))\nX_train.age = std.fit_transform(X_train.age.values.reshape(-1, 1))\nX_test.age = std.fit_transform(X_test.age.values.reshape(-1, 1))\n\nnorm = preprocessing.MinMaxScaler()\nX_train.reward = norm.fit_transform(X_train.reward.values.reshape(-1, 1))\nX_train.difficulty = norm.fit_transform(X_train.difficulty.values.reshape(-1, 1))\nX_train.reg_month = norm.fit_transform(X_train.reg_month.values.reshape(-1, 1))\nX_test.reward = norm.transform(X_test.reward.values.reshape(-1, 1), )\nX_test.difficulty = norm.transform(X_test.difficulty.values.reshape(-1, 1))\nX_test.reg_month = norm.transform(X_test.reg_month.values.reshape(-1, 1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#X_train and X_test indexes are out of order so reset the indexes. Otherwise, there will be NaN values in the dataset after encoding.\nX_train.reset_index(inplace=True)\nX_test.reset_index(inplace=True)\n\nX_train = X_train.drop(['index'], axis=1)\nX_test = X_test.drop(['index'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test.shape, X_train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modeling\n\nWe already know that our dataset is imbalanced but we may still try to train and classify on the same dataset and check how rightly the classes are classified. If the minority classes are only misclassified, then we may perform some techniques to balance the dataset and may try to increase the recall of the minority classes and the precision of the majority classes. If the model is not even able to predict the majority classes correctly, then we may keep the dataset as it is but fine tune the model.\n\nAs mentioned earlier, since this is a classification problem, the evaluation metric 'accuracy' is not a good choice as accuracy can be a useful measure only if we have the same amount of samples per class but we have an imbalanced set. Other metrics like precision, recall, f1-score are by itself suitable only for binary classification but we could use the same for multiclass classification by the 'one vs all' method, but still it may not work well for an imbalanced dataset. The best metric option available for imbalanced dataset multiclass classification problems are 'confusion matrix', 'macro averaging' and 'micro averaging' of the earlier basic metrics. In 'macro-averaging', we average the performances, e.g., precision or f1-score of each individual class. In 'micro averaging', we calculate the performance, e.g., precision, from the individual true positives, true negatives, false positives, and false negatives of the k-class model. And hence, 'micro averaging' is the best choice to go with.\n"},{"metadata":{},"cell_type":"markdown","source":"## 1. DNN (Benchmark Models)\nLet's use the DNN model trained and tested on the Starbucks dataset as the\nbenchmark model. Models built using DNN are considered as the benchmark because it\ncan perform well on huge datasets and it’s well suited for multiclass classification. As\nwe already know, DNNs can only accept numerical inputs, so we may have to encode\neach categorical input into numerical input. Our dataset has two categorical features:\n'gender’ and ‘offer_id’, which are label encoded, i.e. since the variable ‘offer_id’ has 10\ncategories, each category is denoted by any unique numbers from 0 to 9. The problem\nof using label encoding in DNN is that sometimes the model may learn a false relation\nbetween the categories as they are represented in numbers, i.e. when the category -1\nrepresented as 0 is compared to the category-2 represented as 10, since there is a\ncomparable relation between the numbers 0 and 10, the model may assume that there\nis the same relation between the categories 1 and 2, which is obviously misleading.\nThus, label encoding is only recommended when there is a known relationship between\nthe categories or labels. So now, we have the option to still use the label encoding, or to\nuse any other encoding schemes like embedding, one hot encoding etc. or to use all the\nearlier mentioned schemes and find the respective model performance. Let’s go with\nthe latter and create a larger space for comparison to our model.\n\n### 1.1. Label Encoded categorical data. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense, Activation, Dropout\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\nfrom keras.utils import plot_model\nimport tensorflow as tf\nimport keras\nimport itertools","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"classes=['offer recieved', 'offer viewed', 'transaction', 'offer completed', 'green flag']\n\nmodel = Sequential([Dense(32, input_dim=7, activation='relu'),\n                    Dropout(0.3),\n                    Dense(16, activation='relu'),\n                    Dense(5, activation='softmax')])\n\nmodel.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nhistory = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=15, batch_size=15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=[10, 6])\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.xlabel('epoch')\nplt.ylabel('accuracy')\nplt.legend(['train', 'test'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=[10, 6])\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.xlabel('epoch')\nplt.ylabel('loss')\nplt.legend(['train', 'test'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted = model.predict(X_test)\npredicted = np.argmax(predicted, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f1_score(y_test, predicted, average='micro'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    # print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=90)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cnf_matrix = confusion_matrix(y_test, predicted)\n\n# Plot normalized confusion matrix\nfig = plt.figure(figsize=[10, 8])\nplot_confusion_matrix(cnf_matrix, classes=np.asarray(classes), normalize=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Majority classes are performing well but the minorities are not.\n\nProblem of imbalanced dataset can be inferred from the confusion matrix. Most of the events are wrongly predicted as 'offer completed'; offer completed is the most occuring event or class."},{"metadata":{},"cell_type":"markdown","source":"### 1.2. Embedded categorical data. "},{"metadata":{},"cell_type":"markdown","source":"We had encoded the offer_id and gender feature columns earlier itself, but there is a chance that the model could find a false relation between the labels while learing/training, as we had used label encoding (This type of encoding is really only appropriate if there is a known relationship between the categories). One-hot encoding of 'offer_id' won't be a good idea as it could lead to dimensionality explosion. So, let's perform entity embedding on this categorical column."},{"metadata":{},"cell_type":"markdown","source":"Since our dataset has both numerical columns and categorical columns, we may build a multi input neural net, one input for each categorical feature, as for the numerical features; all of them will be fed from a single input. "},{"metadata":{"trusted":true},"cell_type":"code","source":"NUMERICAL_COLUMNS = ['age', 'income', 'reward', 'difficulty', 'reg_month']\nCATEGORICAL_COLUMNS = ['offer_id', 'gender']\nmodels = []\ninputs = []\n\nnumeric_features = X_train[NUMERICAL_COLUMNS]\ncategorical_features = X_train[CATEGORICAL_COLUMNS]\n\nfor cat in categorical_features:\n    vocab_size = data[cat].nunique()\n    inpt = tf.keras.layers.Input(shape=(1,), name='input_' + '_'.join(cat.split(' ')))\n    \n    embed = tf.keras.layers.Embedding(vocab_size, 200,trainable=True,\\\n                                      embeddings_initializer=tf.random_normal_initializer())(inpt)\n    embed_rehsaped = tf.keras.layers.Reshape(target_shape=(200,))(embed)\n    models.append(embed_rehsaped)\n    inputs.append(inpt)\n    \nnum_input = tf.keras.layers.Input(shape=(len(NUMERICAL_COLUMNS)),\\\n                                  name='input_number_features')\n# append this model to the list of models\nmodels.append(num_input)\n# keep track of the input, we are going to feed them later to the #final model\ninputs.append(num_input)\n\nmerge_models = tf.keras.layers.concatenate(models)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"pre_preds = tf.keras.layers.Dense(128, activation='relu')(merge_models)\npre_preds = tf.keras.layers.Dense(128, activation='relu')(pre_preds)\npre_preds = tf.keras.layers.Dropout(.2)(pre_preds)\npre_preds = tf.keras.layers.Dense(64, activation='relu')(pre_preds)\n\npred = tf.keras.layers.Dense(5, activation='softmax')(pre_preds)\n\nmodel = tf.keras.models.Model(inputs= inputs,\\\n                                       outputs =pred)\nmodel.compile(loss=tf.keras.losses.sparse_categorical_crossentropy,\\\n                       metrics=['accuracy'],\n                       optimizer='adam')\n\n#Since we have used a multi input neural network, it is best practice to feed your train data as a dictionary, \n#where your keys are the name of the Input layer and the values are what each layer is expected to have.\n\ninput_dict = {\n    \"input_offer_id\":X_train[\"offer_id\"],\n    \"input_gender\":X_train[\"gender\"],\n    \"input_number_features\": X_train[NUMERICAL_COLUMNS]\n}\n\ninput_dict_test = {\n    \"input_offer_id\":X_test[\"offer_id\"],\n    \"input_gender\":X_test[\"gender\"],\n    \"input_number_features\": X_test[NUMERICAL_COLUMNS]\n}\n\nhistory = model.fit(input_dict, y_train, epochs=15, validation_data=(input_dict_test, y_test), batch_size=32)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=[10, 6])\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.xlabel('epoch')\nplt.ylabel('accuracy')\nplt.legend(['train', 'test'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=[10, 6])\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.xlabel('epoch')\nplt.ylabel('loss')\nplt.legend(['train', 'test'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted = model.predict(input_dict_test)\npredicted = np.argmax(predicted, axis=1)\nprint(f1_score(y_test, predicted, average='micro'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cnf_matrix = confusion_matrix(y_test, predicted)\n\n# Plot normalized confusion matrix\nfig = plt.figure(figsize=[10, 8])\nplot_confusion_matrix(cnf_matrix, classes=np.asarray(classes), normalize=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Problem of imbalanced dataset can be inferred from the confusion matrix. Most of the events are wrongly predicted as 'offer completed'; offer completed is the most occuring event or class."},{"metadata":{},"cell_type":"markdown","source":"### 1.3. One-Hot Encoded categorical data."},{"metadata":{"trusted":true},"cell_type":"code","source":"enc = preprocessing.OneHotEncoder()\nonehot_df_train = enc.fit_transform(X_train.offer_id.values.reshape(-1, 1))\nonehot_df_test = enc.transform(X_test.offer_id.values.reshape(-1, 1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"onehot_df_train = onehot_df_train.toarray()\nonehot_df_test = onehot_df_test.toarray()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"onehot_df_train = pd.DataFrame(onehot_df_train)\nonehot_df_test = pd.DataFrame(onehot_df_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = pd.concat([X_train, onehot_df_train], axis=1)\nX_test = pd.concat([X_test, onehot_df_test], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = X_train.drop(['offer_id'], axis=1)\nX_test = X_test.drop(['offer_id'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"model = Sequential([Dense(8, input_dim=16, activation='relu'),\n                    Dense(8, activation='relu'),\n                    Dense(5, activation='softmax')])\n\nmodel.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nhistory = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20, batch_size=20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=[10, 8])\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.xlabel('epoch')\nplt.ylabel('accuracy')\nplt.legend(['train', 'test'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=[10, 8])\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.xlabel('epoch')\nplt.ylabel('loss')\nplt.legend(['train', 'test'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted = model.predict(X_test)\npredicted = np.argmax(predicted, axis=1)\nprint(f1_score(y_test, predicted, average='micro'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cnf_matrix = confusion_matrix(y_test, predicted)\n\n# Plot normalized confusion matrix\nfig = plt.figure(figsize=[10, 8])\nplot_confusion_matrix(cnf_matrix, classes=np.asarray(classes), normalize=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Problem of imbalanced dataset can be inferred from the confusion matrix. Most of the events are wrongly predicted as 'offer completed'; offer completed is the most occuring event or class."},{"metadata":{},"cell_type":"markdown","source":"As expected, the label encoded model has performed the least. The feature embedded model has performed better than both the label encoded and one hot encoded model. An embedding is actually a mapping of a discrete categorical variable to a vector of continuous numbers. Entity embedding not only reduces memory usage and speeds up neural networks compared with one-hot encoding, but more importantly by mapping similar values close to each other in the embedding space it reveals the intrinsic properties of the categorical variables. It can rapidly generate great results on structured data without having to resort to feature engineering or apply domain specific knowledge. \n"},{"metadata":{},"cell_type":"markdown","source":"## 2. XGBoost (Our Model)\n\nLet's use the XGBoost machine learning technique to build our model. One could find it less trivial to implement XGBoost for multiclass classification as it’s not directly implemented to the Python API XGBClassifier. To use XGBoost main module for a multiclass classification problem, it is required to change the value of two parameters: objective and num_class. Features are not normalised as this method is based on decision trees.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\nfrom sklearn.metrics import classification_report, confusion_matrix, recall_score, f1_score\nimport itertools","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(predictors, target, test_size=0.25, random_state=111)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#X_train and X_test indexes are out of order so reset the indexes\nX_train.reset_index(inplace=True)\nX_test.reset_index(inplace=True)\n\nX_train = X_train.drop(['index'], axis=1)\nX_test = X_test.drop(['index'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape, X_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {  \n    'max_depth' : 10,\n    'gamma'     : 5,\n    'objective' : 'multi:softmax',\n    'num_class' : 5,\n    'eval_metric' : [\"merror\", 'mlogloss'],\n    'n_gpus' : 0\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = xgb.XGBClassifier(**params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"evallist = [(X_train, y_train), (X_test, y_test)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(X_train, y_train, eval_set=evallist, eval_metric=[\"merror\", 'mlogloss'], verbose=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# retrieve performance metrics\nresults = model.evals_result_\nepochs = len(results['validation_0']['merror'])\nx_axis = range(0, epochs)\nfig, ax = plt.subplots(figsize=(10,8))\nplt.plot(x_axis, results['validation_0']['merror'], label = 'Train')\nplt.plot(x_axis, results['validation_1']['merror'], label = 'Test')\nax.legend()\nplt.ylabel('Classification Error')\nplt.title('XGBoost Classification Error')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# retrieve performance metrics\nresults = model.evals_result_\nepochs = len(results['validation_0']['mlogloss'])\nx_axis = range(0, epochs)\nfig, ax = plt.subplots(figsize=(10,8))\nplt.plot(x_axis, results['validation_0']['mlogloss'], label = 'Train')\nplt.plot(x_axis, results['validation_1']['mlogloss'], label = 'Test')\nax.legend()\nplt.ylabel('Classification Loss')\nplt.title('XGBoost Classification Loss')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f1_score(y_test, predictions, average='micro'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cnf_matrix = confusion_matrix(y_test, predictions)\n\n# Plot normalized confusion matrix\nfig = plt.figure(figsize=[10, 8])\nplot_confusion_matrix(cnf_matrix, classes=np.asarray(classes), normalize=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the confusion matrix it's evident that the classes are wrongly classified as our dataset is imbalanced. The least occuring classes, 'offer viewed' and 'green flag' are the ones which are mostly wrongly predicted or classified."},{"metadata":{},"cell_type":"markdown","source":"# Handling Imbalanced Dataset\n\nWith the imbalanced dataset, we are getting a micro-average f1-score of around 63.45% for the XGB model; In reality this value cannot be taken into consideration for determining the performance of the model as that value would be more representative of the majority class, i.e. the true predictions of the majority classes 'offer received', 'offer transaction' and 'offer completed' are 0.48, 0.57 and 0.87 respectively; where as the true predictions of the minority classes 'offer viewed' and 'green flag' are 0.0 and 0.07 respectively; Thus, if we classify the records from the class 'offer viewed' by just seeing the f1-score of the model, then obviously the model is never going to predict it correct as that class is having a true score of exact zero.\n\n\nThe micro-average f1-score of different models on imbalanced dataset are:\n\n - DNN (Label Enoded categorical data) &emsp;  &ensp; &nbsp; - 61.85%\n - DNN (Embedded categorical data) &emsp; &emsp; &ensp; &nbsp; - 63.03%\n - DNN (One-Hot Encoded categorical data) &ensp; - 62.50%\n - XGBoost (Non-Normalized) &emsp; &emsp; &emsp; &emsp; &emsp; &ensp; - 63.45%\n \n##### From the confusion matrices we can infer that most of the wrong classifications are made from the minority classes. And the majority classes are having higher rate of true predictions. Thus, it's comprehensible that we can get higher true predictions from the same model if we have more datapoints or records for the minority classes. Getting new intended data from the customers would not be a hardship for Starbucks, when we already have a model which can perform well with enough data. But, for the time being we may use imbalanced dataset handling techniques on the dataset to prove that we can get atleast average true predictions for all the classes with just these sampled data.\n\nIt's known that the newly sampled data can never represent the original data, hence we aren't expecting to get a greater accuracy or precision but just to check whether an average amount of data can be classified into its true classes; If yes, more data of the minority classes from starbucks in the future can make the model much more accurate; Otherwise, we may have to understand that wrong classification are because the datapoints or records of 'offer viewed' and 'green flag' are so similar such that the model cannot distinguish among them and even new data cannot help the model to perform well.\n \nNow, let's treat the imbalanced dataset by using the RandomOverSampler (Random Over-sampling Technique) for over-sampling and see how the confusion matrix changes for the models."},{"metadata":{},"cell_type":"markdown","source":"## 1. Random Over-Sampling - XGBoost"},{"metadata":{},"cell_type":"markdown","source":"Ensure that we apply oversampling technique only on the train set, because oversampling on the dataset may allow the exact same observations to be present in both the test and train sets. This can make our model to simply memorize specific data points and cause overfitting and poor generalization to the test data.\nNow, let's treat the imbalanced dataset by using the RandomOverSampler (Random Over-sampling Technique) for over-sampling and see how the confusion matrix changes for the benchmark model and our model.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from imblearn.over_sampling import RandomOverSampler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sm = RandomOverSampler(sampling_strategy='not majority')\nX_train, y_train = sm.fit_sample(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {\n    'max_depth' : 14,\n    'gamma'     : 5,\n    'objective' : 'multi:softmax',\n    'num_class' : 5,\n    'eval_metric' : [\"merror\", 'mlogloss'],\n    'n_gpus' : 0\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = xgb.XGBClassifier(**params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"evallist = [(X_train, y_train), (X_test, y_test)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(X_train, y_train, eval_set=evallist, eval_metric=[\"merror\", 'mlogloss'], verbose=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# retrieve performance metrics\nresults = model.evals_result_\nepochs = len(results['validation_0']['merror'])\nx_axis = range(0, epochs)\nfig, ax = plt.subplots(figsize=(10,8))\nplt.plot(x_axis, results['validation_0']['merror'], label = 'Train')\nplt.plot(x_axis, results['validation_1']['merror'], label = 'Test')\nax.legend()\nplt.ylabel('Classification Loss')\nplt.title('XGBoost Classification Loss')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# retrieve performance metrics\nresults = model.evals_result_\nepochs = len(results['validation_0']['mlogloss'])\nx_axis = range(0, epochs)\nfig, ax = plt.subplots(figsize=(10,8))\nplt.plot(x_axis, results['validation_0']['mlogloss'], label = 'Train')\nplt.plot(x_axis, results['validation_1']['mlogloss'], label = 'Test')\nax.legend()\nplt.ylabel('Classification Loss')\nplt.title('XGBoost Classification Loss')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(recall_score(y_test, predictions, average='micro'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cnf_matrix = confusion_matrix(y_test, predictions)\n\n# Plot normalized confusion matrix\nfig = plt.figure(figsize=[10, 8])\nplot_confusion_matrix(cnf_matrix, classes=np.asarray(classes), normalize=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### As we thought, the model is able to classify 50% of all the records from each class correctly. As said earlier, we should not be concerned about the classification score right now, as we already know that the sampled data cannot give much precision as compared to the original data. "},{"metadata":{},"cell_type":"markdown","source":"## Random Over-Sampling - DNN"},{"metadata":{"trusted":true},"cell_type":"code","source":"std = preprocessing.StandardScaler()\nX_train.income = std.fit_transform(X_train.income.values.reshape(-1, 1))\nX_test.income = std.transform(X_test.income.values.reshape(-1, 1))\nX_train.age = std.fit_transform(X_train.age.values.reshape(-1, 1))\nX_test.age = std.fit_transform(X_test.age.values.reshape(-1, 1))\n\nnorm = preprocessing.MinMaxScaler()\nX_train.reward = norm.fit_transform(X_train.reward.values.reshape(-1, 1))\nX_train.difficulty = norm.fit_transform(X_train.difficulty.values.reshape(-1, 1))\nX_train.reg_month = norm.fit_transform(X_train.reg_month.values.reshape(-1, 1))\nX_test.reward = norm.transform(X_test.reward.values.reshape(-1, 1), )\nX_test.difficulty = norm.transform(X_test.difficulty.values.reshape(-1, 1))\nX_test.reg_month = norm.transform(X_test.reg_month.values.reshape(-1, 1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"NUMERICAL_COLUMNS = ['age', 'income', 'reward', 'difficulty', 'reg_month']\nCATEGORICAL_COLUMNS = ['offer_id', 'gender']\nmodels = []\ninputs = []\n\nnumeric_features = X_train[NUMERICAL_COLUMNS]\ncategorical_features = X_train[CATEGORICAL_COLUMNS]\n\nfor cat in categorical_features:\n    vocab_size = data[cat].nunique()\n    inpt = tf.keras.layers.Input(shape=(1,), name='input_' + '_'.join(cat.split(' ')))\n    \n    embed = tf.keras.layers.Embedding(vocab_size, 200,trainable=True,\\\n                                      embeddings_initializer=tf.random_normal_initializer)(inpt)\n    embed_rehsaped = tf.keras.layers.Reshape(target_shape=(200,))(embed)\n    models.append(embed_rehsaped)\n    inputs.append(inpt)\n    \nnum_input = tf.keras.layers.Input(shape=(len(NUMERICAL_COLUMNS)),\\\n                                  name='input_number_features')\n# append this model to the list of models\nmodels.append(num_input)\n# keep track of the input, we are going to feed them later to the #final model\ninputs.append(num_input)\n\nmerge_models = tf.keras.layers.concatenate(models)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"pre_preds = tf.keras.layers.Dense(32, activation='relu')(merge_models)\npre_preds = tf.keras.layers.Dense(32, activation='relu')(pre_preds)\npre_preds = tf.keras.layers.Dropout(.2)(pre_preds)\npre_preds = tf.keras.layers.Dense(32, activation='relu')(pre_preds)\n\npred = tf.keras.layers.Dense(5, activation='softmax')(pre_preds)\n\nmodel = tf.keras.models.Model(inputs= inputs,\\\n                                       outputs =pred)\nmodel.compile(loss=tf.keras.losses.sparse_categorical_crossentropy,\\\n                       metrics=['accuracy'],\n                       optimizer='adam')\n\n#Since we have used a multi input neural network, it is best practice to feed your train data as a dictionary, \n#where your keys are the name of the Input layer and the values are what each layer is expected to have.\n\ninput_dict= {\n    \"input_offer_id\":X_train[\"offer_id\"],\n    \"input_gender\":X_train[\"gender\"],\n    \"input_number_features\": X_train[NUMERICAL_COLUMNS]\n}\n\ninput_dict_test= {\n    \"input_offer_id\":X_test[\"offer_id\"],\n    \"input_gender\":X_test[\"gender\"],\n    \"input_number_features\": X_test[NUMERICAL_COLUMNS]\n}\n\nhistory = model.fit(input_dict, y_train, epochs=15, validation_data=(input_dict_test, y_test), batch_size=64)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=[10, 6])\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.xlabel('epoch')\nplt.ylabel('accuracy')\nplt.legend(['train', 'test'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=[10, 6])\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.xlabel('epoch')\nplt.ylabel('loss')\nplt.legend(['train', 'test'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted = model.predict(input_dict_test)\npredicted = np.argmax(predicted, axis=1)\nprint(f1_score(y_test, predicted, average='micro'))","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"cnf_matrix = confusion_matrix(y_test, predicted)\n\n# Plot normalized confusion matrix\nfig = plt.figure(figsize=[10, 8])\nplot_confusion_matrix(cnf_matrix, classes=np.asarray(classes), normalize=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Class Weight Adjustment - DNN\n"},{"metadata":{},"cell_type":"markdown","source":"In this method, we may provide a weight for each class which places more emphasis on the minority classes such that the end result is a classifier which can learn equally from all classes. The class weights can be set by assigning a dictionary of class number and its corresponding weights to the argument ‘class_weight’ of fit function provided in keras.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"class_weights = {\n    0 : 3.2,\n    1 : 39.,\n    2 : 1.4,\n    3 : 1.,\n    4 : 6.\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"NUMERICAL_COLUMNS = ['age', 'income', 'reward', 'difficulty', 'reg_month']\nCATEGORICAL_COLUMNS = ['offer_id', 'gender']\nmodels = []\ninputs = []\n\nnumeric_features = X_train[NUMERICAL_COLUMNS]\ncategorical_features = X_train[CATEGORICAL_COLUMNS]\n\nfor cat in categorical_features:\n    vocab_size = data[cat].nunique()\n    inpt = tf.keras.layers.Input(shape=(1,), name='input_' + '_'.join(cat.split(' ')))\n    \n    embed = tf.keras.layers.Embedding(vocab_size, 200,trainable=True,\\\n                                      embeddings_initializer=tf.random_normal_initializer)(inpt)\n    embed_rehsaped = tf.keras.layers.Reshape(target_shape=(200,))(embed)\n    models.append(embed_rehsaped)\n    inputs.append(inpt)\n    \nnum_input = tf.keras.layers.Input(shape=(len(NUMERICAL_COLUMNS)),\\\n                                  name='input_number_features')\n# append this model to the list of models\nmodels.append(num_input)\n# keep track of the input, we are going to feed them later to the #final model\ninputs.append(num_input)\n\nmerge_models = tf.keras.layers.concatenate(models)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"pre_preds = tf.keras.layers.Dense(64, activation='relu')(merge_models)\npre_preds = tf.keras.layers.Dense(64, activation='relu')(pre_preds)\npre_preds = tf.keras.layers.Dropout(.2)(pre_preds)\npre_preds = tf.keras.layers.Dense(32, activation='relu')(pre_preds)\n\npred = tf.keras.layers.Dense(5, activation='softmax')(pre_preds)\n\nmodel = tf.keras.models.Model(inputs= inputs,\\\n                                       outputs =pred)\nmodel.compile(loss=tf.keras.losses.sparse_categorical_crossentropy,\\\n                       metrics=['accuracy'],\n                       optimizer='adam')\n\n#Since we have used a multi input neural network, it is best practice to feed your train data as a dictionary, \n#where your keys are the name of the Input layer and the values are what each layer is expected to have.\n\ninput_dict= {\n    \"input_offer_id\":X_train[\"offer_id\"],\n    \"input_gender\":X_train[\"gender\"],\n    \"input_number_features\": X_train[NUMERICAL_COLUMNS]\n}\n\ninput_dict_test= {\n    \"input_offer_id\":X_test[\"offer_id\"],\n    \"input_gender\":X_test[\"gender\"],\n    \"input_number_features\": X_test[NUMERICAL_COLUMNS]\n}\n\nhistory = model.fit(input_dict, y_train, epochs=15, validation_data=(input_dict_test, y_test), batch_size=32, class_weight=class_weights)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=[10, 6])\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.xlabel('epoch')\nplt.ylabel('accuracy')\nplt.legend(['train', 'test'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=[10, 6])\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.xlabel('epoch')\nplt.ylabel('loss')\nplt.legend(['train', 'test'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted = model.predict(input_dict_test)\npredicted = np.argmax(predicted, axis=1)\nprint(f1_score(y_test, predicted, average='micro'))","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"cnf_matrix = confusion_matrix(y_test, predicted)\n\n# Plot normalized confusion matrix\nfig = plt.figure(figsize=[10, 8])\nplot_confusion_matrix(cnf_matrix, classes=np.asarray(classes), normalize=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Random Forest for Imbalanced Classification\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(predictors, target, test_size=0.25, random_state=111)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = X_train.drop(['offer_id'], axis=1)\nX_test = X_test.drop(['offer_id'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = RandomForestClassifier(n_estimators=150, class_weight='balanced_subsample')\nhistory = model.fit(X_train, y_train)\nprediction = model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f1_score(y_test, prediction, average='micro'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cnf_matrix = confusion_matrix(y_test, prediction)\n\n# Plot normalized confusion matrix\nfig = plt.figure(figsize=[10, 8])\nplot_confusion_matrix(cnf_matrix, classes=np.asarray(classes), normalize=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. K-Fold Cross-Validation"},{"metadata":{},"cell_type":"markdown","source":"Cross-validation is primarily used in applied machine learning to estimate the skill of a machine learning model on unseen data. That is, to use a limited sample in order to estimate how the model is expected to perform in general when used to make predictions on data not used during the training of the model. We may use the Stratified-K-Fold cross-validation method to test the effectiveness of our XGBoost model. Stratified-K-fold is best suited for imbalanced dataset classification so as in our case. It returns stratified folds, i.e while making the folds it maintains the percentage of samples for each class in every fold. So that model gets equally distributed data for training/test folds. I have set the parameter ‘K’ to 10 i.e. we may split the dataset into 10 parts or folds. Each time on the loop we may consider a single fold for testing and rest for training, non-repeatedly. We may evaluate the model on micro-average f1-score each time on the respective test set and all the scores are averaged to obtain a more comprehensive model validation score.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# scikit-learn k-fold cross-validation\nfrom numpy import array\nfrom sklearn.model_selection import StratifiedKFold","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictors_temp = predictors\npredictors_temp = np.array(predictors_temp)\ntarget_temp = target\ntarget_temp = np.array(target_temp)\nvalidation_scores = list()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" params = {\n    \n    'max_depth' : 10,\n    'gamma'     : 5,\n    'objective' : 'multi:softmax',\n    'num_class' : 5,\n    'eval_metric' : [\"merror\", 'mlogloss'],\n    'n_gpus' : 0,\n    'n_estimators' : 100\n     \n          }\n    \ndef create_and_validate_model(X_train, y_train, X_test, y_test):\n    model = xgb.XGBClassifier(**params)\n    evallist = [(X_train, y_train), (X_test, y_test)]\n    model.fit(X_train, y_train, eval_set=evallist, eval_metric=[\"merror\", 'mlogloss'], verbose=False)\n    predictions = model.predict(X_test)\n    validation_scores.append(f1_score(y_test, predictions, average='micro'))\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"skfold = StratifiedKFold(n_splits=10, random_state=111)\nfor train_index, test_index in skfold.split(predictors_temp, target_temp):\n    X_train, X_test = predictors_temp[train_index], predictors_temp[test_index]\n    y_train, y_test = target_temp[train_index], target_temp[test_index]\n    create_and_validate_model(X_train, y_train, X_test, y_test)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(validation_scores)\nprint(np.mean(validation_scores))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Average validation score after cross-validation is 63.62, which is quite impressive as our\nmodel is having a similar micro-average f1-score of 63.45. And, from the validation\nscore of each model, we can infer that there is much less variance among them, which’s\na positive indication. A good model is not the one that gives accurate predictions on the\nknown data or training data but the one which gives good predictions on the new data\nand avoids overfitting and underfitting. Thus, we may possibly consider our model as a\ngood model. (Keeping the problems due to the imbalanced dataset aside.)"},{"metadata":{},"cell_type":"markdown","source":"## 3. SHAP"},{"metadata":{},"cell_type":"markdown","source":"The SHAP allows us to show how much each predictor contributes, either positively or negatively, to the target variable. This is like the variable importance plot but it is able to show the positive or negative relationship for each variable with the target. We may use the Tree Explainer function available in SHAP for interpreting our XGBoost machine learning model. Since our’s is a multi-class classification model, we have to show the model’s multiple outputs for a single observation. This means we may have 5 plots (since we have 5 classes) to look at instead of just one (as in the case of binary classification and regression problems). This is useful because we could know why the model made a decision as they are in and why it didn't make another. \n"},{"metadata":{},"cell_type":"markdown","source":"### 3.1 Force Plot"},{"metadata":{"trusted":true},"cell_type":"code","source":"import shap","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#attempt to use SHAP on multi-class\nX_rand = predictors.sample(1, random_state=42)\nidx = X_rand.index.values[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_rand","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(target[X_rand.index])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#define the explainer and find force plot for each class on the same sample\nexplainer = shap.TreeExplainer(model)\nshap_values = explainer.shap_values(predictors.iloc[idx])\nshap.initjs()\nfor which_class in range(0,5):\n    display(shap.force_plot(explainer.expected_value[which_class], shap_values[which_class], X_rand))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###  3.2 Variable Importance Plot"},{"metadata":{},"cell_type":"markdown","source":"A summary plot with plot type equal to bar or variable importance plot lists the most significant variables in descending order. The top variables contribute more to the model than the bottom ones and thus have high predictive power. Since our’s is a multi-class classification model, we have stacked bars for each feature.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"shap_values = shap.TreeExplainer(model).shap_values(X_rand)\nshap.summary_plot(shap_values, X_rand)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"we can observe that the variable ‘offer_id’ is influencing the model\nthe most; whereas the variable ‘reg_month’ is influencing the model the least. And also,\nwe can infer the stacked bars which indicate the predictive power of each feature on\neach class. The variable offer_id has a greater impact on the target ‘Class 0’, whereas it\nhas least impact on the target ‘Class 1’. It’s strange to see that the variable ‘income’\ndoesn't have any influence on the target ‘Class 0’."},{"metadata":{},"cell_type":"markdown","source":"## 2. Dropping Least Occuring Classes "},{"metadata":{},"cell_type":"markdown","source":"We have seen how model perform on the imbalanced data and balanced data through sampling. Let's try another way of understanding how well the model performs; Let's try classifying the majority classes only by dropping the minority classes. We expect a greater classification score than that of random over-sampled XGBoost model, as we don't have any sampled data anymore."},{"metadata":{"trusted":true},"cell_type":"code","source":"filter_1 = data['event_id'] != 1\nfilter_2 = data['event_id'] != 4\n\ndata_after_filter = data.where(filter_1 & filter_2, axis=0)\ndata_after_filter.dropna(inplace=True)\ndata_after_filter.reset_index(inplace=True)\ntarget_filtered = data_after_filter['event_id']\npredictor_filtered = data_after_filter.drop(['index', 'event_id', 'id'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_filtered, X_test_filtered, y_train_filtered, y_test_filtered = train_test_split(predictor_filtered, target_filtered, test_size=.25)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {\n    \n    'max_depth' : 5,\n    'objective' : 'multi:softmax',\n    'num_class' : 3,\n    'eval_metric' : [\"merror\", 'mlogloss'],\n    'n_gpus' : 0,\n    'n_estimators' : 60\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = xgb.XGBClassifier(**params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"evallist = [(X_train_filtered, y_train_filtered), (X_test_filtered, y_test_filtered)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(X_train_filtered, y_train_filtered, eval_set=evallist, eval_metric=[\"merror\", 'mlogloss'], verbose=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = model.predict(X_test_filtered)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# retrieve performance metrics\nresults = model.evals_result_\nepochs = len(results['validation_0']['mlogloss'])\nx_axis = range(0, epochs)\nfig, ax = plt.subplots(figsize=(10,8))\nplt.plot(x_axis, results['validation_0']['mlogloss'], label = 'Train')\nplt.plot(x_axis, results['validation_1']['mlogloss'], label = 'Test')\nax.legend()\nplt.ylabel('Classification Loss')\nplt.title('XGBoost Classification Loss')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# retrieve performance metrics\nresults = model.evals_result_\nepochs = len(results['validation_0']['merror'])\nx_axis = range(0, epochs)\nfig, ax = plt.subplots(figsize=(10,8))\nplt.plot(x_axis, results['validation_0']['merror'], label = 'Train')\nplt.plot(x_axis, results['validation_1']['merror'], label = 'Test')\nax.legend()\nplt.ylabel('Classification Error')\nplt.title('XGBoost Classification Error')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f1_score(y_test_filtered, predictions, average='micro'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test_filtered, predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classes_filtered = ['offer recieved', 'transaction', 'offer completed']\n\ncnf_matrix = confusion_matrix(y_test_filtered, predictions)\n\n#plot normalized confusion matrix\nfig = plt.figure(figsize=[10, 8])\nplot_confusion_matrix(cnf_matrix, classes=np.asarray(classes_filtered), normalize=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As mentioned earlier, we weren't expecting to get a greater accuracy, recall or precision for the model (As we’re using imbalanced dataset) but we were to check whether an average amount of data can be classified into its true classes; since now we know it's possible, more data of the minority classes from starbucks in the future can make the model perform much more accurately. Otherwise, we have to understand that the wrong classifications are because the data points or records of each class are so similar that the model is not able to distinguish among them and even a new set of data records cannot make the model perform well. To our expectation, we were able to prove that half the amount of records from each class are classified correctly. And, using our model evaluation framework, we made it clear that a higher accuracy is possible in the future with enough data. (The majority class was classified with a recall of 87%) \n\n# Conclusion\nProblem of imbalanced dataset can be inferred from the confusion matrices of the benchmark models 1\nto 3 and our Model, that is, all the minority classes are wrongly predicted as\n'offer completed', which is the most occuring event or majority class. In\nessence, it is possibly because the models have over learned the majority class and\nthey aren't anymore able to predict the minority classes. This problem is handled in Model 4\nto Model 6 using imbalanced dataset handling techniques. The smooth growth in learning curve and abrupt or no growth in validation curve of Model 4 indicates that the test set is not a representative of the oversampled data. At the same time, the confusion matrix of the Model 5 built using XGBoost on the same oversampled data indicates that 50% of the records from each class are correctly\nclassified and is working well as expected. From the confusion matrix of Model 6, we can infer that the\nmodel has done pretty well when compared to Model 4 but not as good as Model 5 where class ‘offer viewed’, which is having the least number of data points of all is not predicted at all, which is possibly\nbecause the data points are so little that even the class weight adjustment couldn't prioratize it. The Model 7 is essentially an evaluation framework where we have dropped the minority classes and have made the model to learn and validate on the\nmajority class alone. Hence why the model witnessed an increase in micro-average f1-score\ncompared to all other models, even after new dataset being imbalanced again.\n"},{"metadata":{},"cell_type":"markdown","source":"# End"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}